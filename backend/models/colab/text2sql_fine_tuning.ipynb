{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "E5Z698ntsztk"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers datasets accelerate huggingface_hub torch sentencepiece\n",
        "!pip install -q bitsandbytes  # For efficient training\n",
        "\n",
        "# Login to HuggingFace (you'll be prompted for your token)\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Method 1: Upload individual files (if small)\n",
        "print(\"Upload the 4 Spider JSON files when prompted...\")\n",
        "uploaded = files.upload()  # Click 'Choose Files' and select all 4 files\n",
        "\n",
        "# Create spider directory\n",
        "os.makedirs('/content/spider', exist_ok=True)\n",
        "\n",
        "# Move uploaded files\n",
        "for filename in uploaded.keys():\n",
        "    os.rename(filename, f'/content/spider/{filename}')\n",
        "    print(f\"✓ Moved {filename} to /content/spider/\")\n",
        "\n",
        "print(\"\\n✓ All files uploaded successfully!\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SrbnU_WYtlPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Verify Spider files are present\n",
        "import os\n",
        "spider_path = '/content/spider'\n",
        "required_files = ['tables.json', 'train_spider.json', 'train_others.json', 'dev.json']\n",
        "\n",
        "print(\"Checking Spider files...\")\n",
        "for f in required_files:\n",
        "    full_path = os.path.join(spider_path, f)\n",
        "    exists = os.path.exists(full_path)\n",
        "    print(f\"  {'✓' if exists else '✗'} {f}\")\n",
        "    if not exists:\n",
        "        raise FileNotFoundError(f\"Missing: {f}\")\n",
        "\n",
        "print(\"\\n✓ All required files found!\\n\")\n",
        "\n",
        "# Load Spider data\n",
        "def load_spider_dataset(base_path='/content/spider'):\n",
        "    \"\"\"Load Spider train, dev, and tables.json\"\"\"\n",
        "\n",
        "    # Load schemas\n",
        "    with open(f'{base_path}/tables.json', 'r') as f:\n",
        "        tables = json.load(f)\n",
        "\n",
        "    # Create schema lookup\n",
        "    schema_dict = {}\n",
        "    for db in tables:\n",
        "        db_id = db['db_id']\n",
        "        tables_list = []\n",
        "\n",
        "        table_names = db.get('table_names_original', db.get('table_names', []))\n",
        "        column_names = db.get('column_names_original', db.get('column_names', []))\n",
        "        column_types = db.get('column_types', [])\n",
        "\n",
        "        # Group columns by table\n",
        "        table_columns = {}\n",
        "        for idx, (table_idx, col_name) in enumerate(column_names):\n",
        "            if table_idx == -1:\n",
        "                continue\n",
        "            table_name = table_names[table_idx]\n",
        "            if table_name not in table_columns:\n",
        "                table_columns[table_name] = []\n",
        "            col_type = column_types[idx] if idx < len(column_types) else 'text'\n",
        "            table_columns[table_name].append(f\"{col_name} ({col_type})\")\n",
        "\n",
        "        # Format schema text\n",
        "        schema_text = f\"Database: {db_id}\\\\nTables:\\\\n\"\n",
        "        for table in table_names:\n",
        "            cols = table_columns.get(table, [])\n",
        "            schema_text += f\"  - {table}: {', '.join(cols)}\\\\n\"\n",
        "\n",
        "        schema_dict[db_id] = schema_text.strip()\n",
        "\n",
        "    # Load training examples\n",
        "    def load_examples(file_path):\n",
        "        with open(file_path, 'r') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    train_spider = load_examples(f'{base_path}/train_spider.json')\n",
        "    train_others = load_examples(f'{base_path}/train_others.json')\n",
        "    dev = load_examples(f'{base_path}/dev.json')\n",
        "\n",
        "    # Combine all training data\n",
        "    all_train = train_spider + train_others + dev\n",
        "\n",
        "    print(f\"Loaded {len(all_train)} total examples\")\n",
        "    print(f\"  - train_spider: {len(train_spider)}\")\n",
        "    print(f\"  - train_others: {len(train_others)}\")\n",
        "    print(f\"  - dev: {len(dev)}\")\n",
        "\n",
        "    # Format for fine-tuning\n",
        "    formatted_data = []\n",
        "    for ex in all_train:\n",
        "        question = ex['question']\n",
        "        sql = ex['query']\n",
        "        db_id = ex['db_id']\n",
        "        schema = schema_dict.get(db_id, '')\n",
        "\n",
        "        # Input: schema + question (simple format matching our prompt)\n",
        "        input_text = f\"{schema}\\\\n\\\\nQ: {question}\\\\nA:\"\n",
        "        target_text = sql\n",
        "\n",
        "        formatted_data.append({\n",
        "            'input': input_text,\n",
        "            'target': target_text,\n",
        "            'db_id': db_id,\n",
        "            'question': question\n",
        "        })\n",
        "\n",
        "    return formatted_data, schema_dict\n",
        "\n",
        "\n",
        "# Load data\n",
        "train_data, schemas = load_spider_dataset('/content/spider')\n",
        "\n",
        "# Create train/validation split (90/10)\n",
        "split_idx = int(len(train_data) * 0.9)\n",
        "train_split = train_data[:split_idx]\n",
        "val_split = train_data[split_idx:]\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "train_dataset = Dataset.from_list(train_split)\n",
        "val_dataset = Dataset.from_list(val_split)\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': val_dataset\n",
        "})\n",
        "\n",
        "print(f\"\\\\nDataset split:\")\n",
        "print(f\"  Train: {len(train_dataset)}\")\n",
        "print(f\"  Validation: {len(val_dataset)}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UhUigEHGt40a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU memory first\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"✓ GPU memory cleared\")"
      ],
      "metadata": {
        "id": "NuVc4YXG1DoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Reload model with gradient checkpointing\n",
        "model_name = \"Salesforce/codet5p-770m\"\n",
        "print(f\"Reloading model with gradient checkpointing...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Enable gradient checkpointing to save memory\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(\"✓ Model reloaded with gradient checkpointing enabled\")\n",
        "\n",
        "# Tokenize datasets\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"Tokenize inputs and targets\"\"\"\n",
        "    inputs = examples['input']\n",
        "    targets = examples['target']\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "\n",
        "    # Tokenize targets\n",
        "    labels = tokenizer(\n",
        "        targets,\n",
        "        max_length=256,\n",
        "        truncation=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset['train'].column_names,\n",
        "    desc=\"Tokenizing dataset\"\n",
        ")\n",
        "\n",
        "print(\"\\\\nTokenization complete!\")"
      ],
      "metadata": {
        "id": "KLYUZt24up3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "# Ultra-low memory configuration\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./codet5p-spider-finetuned\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=1,  # Minimum batch size\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,  # Maintain effective batch size of 16\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=1000,  # Less frequent eval to save memory\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,  # Keep fewer checkpoints\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    fp16=False,\n",
        "    gradient_checkpointing=True,  # Enable in args too\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=False,\n",
        "    remove_unused_columns=False,\n",
        "    max_grad_norm=1.0,  # Gradient clipping\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['validation'],\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"✓ Trainer configured with minimum memory settings\")\n",
        "print(\"Batch size: 1 × 16 gradient accumulation = 16 effective\")\n",
        "print(\"Training will be slower but should fit in memory\")"
      ],
      "metadata": {
        "id": "gKEd_idLw6Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Starting fine-tuning (low memory mode)...\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "aOAqTDwDzCpZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}